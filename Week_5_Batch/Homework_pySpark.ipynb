{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4bf1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0079bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d845e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('homework') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469dcd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6124a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07bd06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('D:/DE-ZoomCamp/week_5_batch/code/fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8be9914d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', StringType(), True), StructField('DOlocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df6d2df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|pickup_datetime|dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+---------------+----------------+------------+------------+-------+----------------------+\n",
      "+--------------------+---------------+----------------+------------+------------+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"SR_Flag\"].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e0d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "756bc2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),  \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PUlocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOlocationID', types.IntegerType(), True), \n",
    "    types.StructField('SR_Flag', types.StringType(), True), \n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d36b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea8b53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09c7edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_october_2019 = df.filter(\n",
    "#     (year(df[\"pickup_datetime\"]) == 2019) & \n",
    "#     (month(df[\"pickup_datetime\"]) == 10) & \n",
    "#     (year(df[\"dropOff_datetime\"]) == 2019) & \n",
    "#     (month(df[\"dropOff_datetime\"]) == 10)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "009ec001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   NULL|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   NULL|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   NULL|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|          82|          82|   NULL|       B00021         |\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|         264|          71|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|         264|          39|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|         264|         188|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|         264|          91|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|         264|          71|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|         264|          35|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|         264|          61|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|         264|         198|   NULL|                B00037|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|         264|         264|   NULL|                  #N/A|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c8091fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b23dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('fhv/2019/10/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec24e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39M\tfhv/2019/10\n",
      "39M\tfhv/2019\n",
      "39M\tfhv\n"
     ]
    }
   ],
   "source": [
    "!du -h fhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d68d1572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 39M\n",
      "-rw-r--r-- 1 Armut 197121    0 Feb 26 17:21 _SUCCESS\n",
      "-rw-r--r-- 1 Armut 197121 6.4M Feb 26 17:21 part-00000-41b48930-19ea-4fbf-9b63-76ec77c36f91-c000.snappy.parquet\n",
      "-rw-r--r-- 1 Armut 197121 6.4M Feb 26 17:21 part-00001-41b48930-19ea-4fbf-9b63-76ec77c36f91-c000.snappy.parquet\n",
      "-rw-r--r-- 1 Armut 197121 6.4M Feb 26 17:21 part-00002-41b48930-19ea-4fbf-9b63-76ec77c36f91-c000.snappy.parquet\n",
      "-rw-r--r-- 1 Armut 197121 6.4M Feb 26 17:21 part-00003-41b48930-19ea-4fbf-9b63-76ec77c36f91-c000.snappy.parquet\n",
      "-rw-r--r-- 1 Armut 197121 6.4M Feb 26 17:21 part-00004-41b48930-19ea-4fbf-9b63-76ec77c36f91-c000.snappy.parquet\n",
      "-rw-r--r-- 1 Armut 197121 6.4M Feb 26 17:21 part-00005-41b48930-19ea-4fbf-9b63-76ec77c36f91-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh fhv/2019/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ccd5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03cd8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "303c20b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhv/2019/10/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78e4d8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armut\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\dataframe.py:330: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('fhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50d6781d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*)\n",
    "FROM\n",
    "    fhv\n",
    "WHERE\n",
    "    DATE(pickup_datetime) = '2019-10-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76800f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39e4bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "565e7817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|duration|\n",
      "+--------+\n",
      "|631152.5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    MAX((unix_timestamp(dropOff_datetime) - unix_timestamp(pickup_datetime)) / 3600) AS duration\n",
    "FROM\n",
    "    fhv\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3979fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "05d52830",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ee2d544d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8f459d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = df_zone \\\n",
    "    .withColumnRenamed('LocationID', 'ZoneLocationID') \\\n",
    "    .withColumnRenamed('Borough', 'ZoneBorough') \\\n",
    "    .drop('service_zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5d5310a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+--------------------+\n",
      "|ZoneLocationID|  ZoneBorough|                Zone|\n",
      "+--------------+-------------+--------------------+\n",
      "|             1|          EWR|      Newark Airport|\n",
      "|             2|       Queens|         Jamaica Bay|\n",
      "|             3|        Bronx|Allerton/Pelham G...|\n",
      "|             4|    Manhattan|       Alphabet City|\n",
      "|             5|Staten Island|       Arden Heights|\n",
      "|             6|Staten Island|Arrochar/Fort Wad...|\n",
      "|             7|       Queens|             Astoria|\n",
      "|             8|       Queens|        Astoria Park|\n",
      "|             9|       Queens|          Auburndale|\n",
      "|            10|       Queens|        Baisley Park|\n",
      "|            11|     Brooklyn|          Bath Beach|\n",
      "|            12|    Manhattan|        Battery Park|\n",
      "|            13|    Manhattan|   Battery Park City|\n",
      "|            14|     Brooklyn|           Bay Ridge|\n",
      "|            15|       Queens|Bay Terrace/Fort ...|\n",
      "|            16|       Queens|             Bayside|\n",
      "|            17|     Brooklyn|             Bedford|\n",
      "|            18|        Bronx|        Bedford Park|\n",
      "|            19|       Queens|           Bellerose|\n",
      "|            20|        Bronx|             Belmont|\n",
      "+--------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0fc77fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone.createOrReplaceTempView(\"df_zone\")\n",
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f2ab1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1dc6bcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+\n",
      "|       Zone|count(ZoneLocationID)|\n",
      "+-----------+---------------------+\n",
      "|Jamaica Bay|                    1|\n",
      "+-----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    df_zone.Zone,\n",
    "    COUNT(df_zone.ZoneLocationID)\n",
    "FROM \n",
    "    df\n",
    "INNER JOIN df_zone ON \n",
    "    df.PUlocationID = df_zone.ZoneLocationID\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 ASC\n",
    "LIMIT \n",
    "    1;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad312ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
